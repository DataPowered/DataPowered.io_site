[{"categories":null,"content":"Background I am a data \u0026 research enthusiast working as Data Scientist at Tesco Bank, following a few years of work as Data Scientist at The Data Lab Innovation Centre. I am widely interested in data analysis, visualisation and manipulation - which I’ve applied across a variety of domains: national health data, occupational therapy, transport, data for good, tourism, and KPI monitoring/internal strategic planning. My academic background is in psychology, emotion and research methods. I have completed my PhD at The University of Edinburgh, investigating if various emotion-generating stimuli used in lab settings could approximate emotional states occurring in daily life (as measured using a bespoke Android phone app). As part of my doctoral research, I also tested if model-based cluster analysis can serve as a computational model for how humans create (fuzzy) emotional categories. Lastly, I have also been heavily involved with community events in the area of data science: I have organised DataTech19 and planned DataTech20; I have also organised the Edinburgh R meetup (EdinbR) for several years. ","date":"2020-03-31","objectID":"/about/:0:1","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Publications Constantinescu, A. C., Wolters, M., Moore, A., \u0026 MacPherson, S. E. (2017). A cluster-based approach to selecting representative stimuli from the International Affective Picture System (IAPS) database. Behavior Research Methods, 49(3), 896-912. ","date":"2020-03-31","objectID":"/about/:0:2","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Links GitHub StackOverflow LinkedIn ","date":"2020-03-31","objectID":"/about/:0:3","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Scottish Government Accelerator Program (August-December 2019) This was a Scottish Government collaborative project aimed at mentoring George Mitchell (Assistant Economist at The Scottish Government) to clean and join financial tax data from a variety of sources, and then display it interactively using a Shiny app. ","date":"2020-03-31","objectID":"/mentorship/:0:1","tags":null,"title":"Mentorship","uri":"/mentorship/"},{"categories":null,"content":"DataFest Social Network Analysis (May-August 2019) This project is part of The Data Lab’s industry placement program, where a data science MSc student will carry out supervised research as part of an internship with an organisation. For this project, I managed Jack Tacchi’s work on dynamic social networks, to see whether changes over time in a particular Twitter network may be related to KPIs such as ticket sales for events. ","date":"2020-03-31","objectID":"/mentorship/:0:2","tags":null,"title":"Mentorship","uri":"/mentorship/"},{"categories":null,"content":"Scottish Government Accelerator Program (April-September 2018) This was a Scottish Government collaborative project, open to employees of the Scottish Government who sought to improve their data skills. I mentored one of the applicants as part of this initiative - Maite Thrower (Senior Analyst from Public Health Intelligence, ISD within NHS / National Services Scotland), and supervised her while learning to create interactive visualisations of medical data. You can read more about this project here. ","date":"2020-03-31","objectID":"/mentorship/:0:3","tags":null,"title":"Mentorship","uri":"/mentorship/"},{"categories":null,"content":"Community engagement Previously (end of 2015 - Nov 2019), I have organised the EdinbR meetup (check out the site for details). EdinbR speakers during this time have included Hadley Wickham and David Robinson. For my community engagement, data skills and commitment to data science work, I was awarded the New Talent DataIQ award (July 2019) SatRday Newcastle conference co-organiser (April 2019) DataTech conference chair (March 2019) ","date":"2020-03-31","objectID":"/engagement/:0:1","tags":null,"title":"Public speaking","uri":"/engagement/"},{"categories":null,"content":"Conference presentations \u0026 invited talks A beginner’s guide to web scraping in Python - an introductory talk demonstrating how to use Scrapy on two example sites (PyData Edinburgh Meetup, Jul 2020) - slides here Putting the science back in data science - a talk on best practices for data collection, preparation and cleaning (Statistics group within Stirling University, Nov 2019) Advice Direct Scotland annual event: I talked about Using Data science for good. The topic focused on using mixed models to assess the impact generated by a charity working with families in need (Sept 2019). Poster presentation on using GAMs on tourism data and a lightning talk on Rasch models at the Royal Statistical Society’s International Conference (Sept 2019) FOSS4G talk on using Shiny and GraphHopper to plot transport routes interactively (Sept 2019) Office of the Chief Economic Adviser (OCEA) invited talk: this talk focused on using generalised additive models with time series data (June 2019) Adjusting reviewer scores for a fairer assessment via multi-faceted Rasch modelling (accepted lightning talk at useR!, June 2019) Generalised Additive Models applied to tourism data (Newcastle Upon Tyne Data Science Meetup, May 2019) - slides here Women in Computing and Data Science (NHS/ISD talk, March 2019) An Introduction to The Data Lab Innovation Centre (Edinburgh Parallel Computing Centre, Jan 2018) Scottish Government Statistical Group R User Day: Good coding practices in R (Oct 2016) ","date":"2020-03-31","objectID":"/engagement/:0:2","tags":null,"title":"Public speaking","uri":"/engagement/"},{"categories":null,"content":"Code demos A swift introduction to interactive visualisations via R and Shiny (Edinburgh Data Visualization Meetup, March 2019) Exploring transport routes, journey characteristics and postcode networks using R Shiny (EdinbR, May 2018) Making sense of Twitter data (EdinbR, Apr 2017) ","date":"2020-03-31","objectID":"/engagement/:0:3","tags":null,"title":"Public speaking","uri":"/engagement/"},{"categories":null,"content":"Podcasts Podcast with John Mertic, Director of Program Management, Linux Foundation (May 2019) The ‘DataFest Special’ podcast: guest speaker discussing DataTech19 (Feb 2019) ","date":"2020-03-31","objectID":"/engagement/:0:4","tags":null,"title":"Public speaking","uri":"/engagement/"},{"categories":["pre-analysis"],"content":"The 80/20 split Since project deadlines tend to be more or less fixed, the extent to which a dataset follows a set of commonly expected guidelines will often determine how much time you have left to spend thinking about your analysis. To use the split that everyone conventionally mentions, you would hope to spend a modest 20% of your time cleaning the data for a project, and 80% planning and carrying out your actual analysis. But often, these numbers might be reversed. Hence a messy, non-standardized dataset can take up most of your time, so that when you finally convert it into a usable format, you realize you have to rush and wrap up your project. Sound familiar? It’s definitely happened to me before. So I’ve started thinking if there is a way to avoid this pattern of work. Whenever you’re not driving your own data collection (which can happen frequently), you have no control over the format your data arrives in. But what you can do is spread the word on what ‘clean’ data means, in the hope that at some point, good practices will become standard and the 80% of time spent on cleaning data will be a thing of the past. Here are some of my ideas for how to improve (or even prevent) messy data. This is not meant to be an exhaustive list, so feel free to pitch in if you think I’ve missed something! It would be really interesting to get other people’s input on this. ","date":"2017-10-18","objectID":"/posts/2017-10-18-data-guidelines/:1:0","tags":["data cleaning"],"title":"Data guidelines for clean and usable data","uri":"/posts/2017-10-18-data-guidelines/"},{"categories":["pre-analysis"],"content":"A list of tidy data recommendations Guideline Details Examples 1 Same code used consistently throughout the database to indicate missing / unknown values. Blank cells should not be used as a substitute, as these can be misleading. Suggestions for missing value codes: “NA” or “999” etc. 2 An accompanying data codebook should be provided. This is a record of all variables within the database, with an explanation for what each of them means, the range of possible values, and the column type (e.g., integer, numeric, categorical etc.) For a variable named ‘ConsumSatisf’, an explanatory entry would be provided in the codebook to say that this variable represents a measure of consumer satisfaction with product ‘X’, and that possible values range from 0 to 9 (with column type as integer). 3 In case multiple datasets are shipped to the analyst, clear indications should be given for whether there is a linking key between these, and what this is. If two datasets are sent, one containing GP referral data, and the other containing specialist care medical data, then the two datasets should contain a common key for patient IDs, which would share the same meaning between dataset (i.e., patient ‘1234567’ should refer to the same person across the two datasets). 4 Removing duplicated rows, i.e., rows that are perfectly identical within the database (from the first, through to the final column). In more sensitive cases, it may not be enough to remove just perfect duplicates, but special care should also be taken when two rows are identical - with only the exception of a few columns. In a database with patient records, a pair of rows is found that share the same patient ID, the same assessment date, the same prescription, and yet one row suggest the patient was discharged, whereas the other suggests the patient was not discharged. Wherever possible, such situations should be avoided before ever shipping the data, as they are misleading. 5 Removing trailing whitespaces. There should be no spaces padding the actual values within cells. A database cell should only contain values such as “3”, rather than “3 ” or “ 3 ” etc. 6 There should only be one ‘atomic’ value per cell. That is, a value coding only one specific characteristic. A single ‘Name’ variable should be replaced by two atomic variables: ‘Surname’ and ‘Forename’. Similarly, a variable called ‘HotelPreferences’ (with a value such as ‘Hilton;Marriot;Ibis’) should be divided into as many separate columns as necessary, e.g., ‘HotelPreference1’ (value = ‘Hilton’), ‘HotelPreference2’ (value = ‘Marriot’), and so on. 7 The same unit of measuremement should be used consistently throughout the same column. If this is not possible, alternatively, the units of measurement can also be mixed, however this can be allowed only if one variable contains the numeric values, and another vartiable specifies the associated unit of measurement. For medication dosages per patient, a variable could be called ‘DailyIbuprofenDosageMg’, and the values in that column should be restricted to only numeric values, e.g., ‘0.5’, ‘1.5’ - which would be known to be in milligrams. Alternatively, if mixing units of measurement, this format is also possible: for a variable called, ‘DailyIbuprofenDosage’, with values ‘0.5’, ‘1.5’ etc, another variable should be created, e.g., ‘DailyIbuprofenUnit’, with values: ‘mg’, ‘mcg’ etc. Of course, the units of measurement would need to be typed in consistentently (e.g., using either ‘mcg’ or ‘micrograms’, but never both interchangeably). 8 Related variables should follow the same naming scheme. Reusing the hotel preferences example above, variables should be named consistently, e.g., ‘HotelPref1’, ‘HotelPref2’ etc, rather than ‘HotelPreference1’, ‘HotelPref2’, ‘Pref3’ etc. 9 Variable names should not contain spaces. A variable name such as ‘Date of birth’ should be replaced with ‘DateOfBirth’ or ‘date_of_birth’ etc. 10 For dates, the same convention should be used consistently and mention","date":"2017-10-18","objectID":"/posts/2017-10-18-data-guidelines/:2:0","tags":["data cleaning"],"title":"Data guidelines for clean and usable data","uri":"/posts/2017-10-18-data-guidelines/"}]